{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import block:\n",
    "import nltk, re, pprint\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from timeit import *\n",
    "#Metrics and testing:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#Models:\n",
    "from sklearn import linear_model \n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "import heapq\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in data:\n",
    "path = \".\"\n",
    "#local paths\n",
    "csv_path = path + \"/background.csv\"\n",
    "train_path = path + \"/train.csv\"\n",
    "meta_path = path + \"/FFMetadata20180221.csv\"\n",
    "pred_path = path + \"/prediction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_csv = pd.read_csv(csv_path, low_memory=False)\n",
    "train_csv = pd.read_csv(train_path)\n",
    "meta_csv = pd.read_csv(meta_path, low_memory=False)\n",
    "\n",
    "# Fix date bug\n",
    "background_csv.cf4fint = ((pd.to_datetime(background_csv.cf4fint) - pd.to_datetime('1960-01-01')) / np.timedelta64(1, 'D')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 13027)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4242 entries, 0 to 4241\n",
      "Columns: 13027 entries, challengeID to k5f1\n",
      "dtypes: float64(544), int64(12368), object(115)\n",
      "memory usage: 421.6+ MB\n",
      "None \n",
      "\n",
      "(2121, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2121 entries, 0 to 2120\n",
      "Data columns (total 7 columns):\n",
      "challengeID         2121 non-null int64\n",
      "gpa                 1165 non-null float64\n",
      "grit                1418 non-null float64\n",
      "materialHardship    1459 non-null float64\n",
      "eviction            1459 non-null float64\n",
      "layoff              1277 non-null float64\n",
      "jobTraining         1461 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 116.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Examine Information\n",
    "print(background_csv.shape)\n",
    "print background_csv.info(), \"\\n\"\n",
    "print(train_csv.shape)\n",
    "print(train_csv.info())\n",
    "#note that object tends to be any data type mixed with NA/str/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 10595)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4242 entries, 0 to 4241\n",
      "Columns: 10595 entries, challengeID to k5f1\n",
      "dtypes: float64(541), int64(9940), object(114)\n",
      "memory usage: 342.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#remove constant variables\n",
    "constants = np.genfromtxt(path + \"/constantVariables.txt\", dtype=str)\n",
    "variables = background_csv.axes[1]\n",
    "not_constants = [i for i, v in enumerate(variables) if v not in constants]\n",
    "background_trim = background_csv.iloc[:,not_constants]\n",
    "print(background_trim.shape)\n",
    "background_trim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "na_bool = background_trim[background_trim.axes[1][0]].isna()\n",
    "print(type(na_bool))\n",
    "#background_trim[background_trim.axes[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336\n"
     ]
    }
   ],
   "source": [
    "#Find variables that are 80% or more neg / NA (missing/blank) values\n",
    "variables = list(background_trim.axes[1])\n",
    "toRemove = []\n",
    "count = 0 \n",
    "for var in variables:\n",
    "    na_bool = background_trim[var].isna()\n",
    "    na_count = sum(na_bool)\n",
    "    \n",
    "    neg_bool = background_trim[var].lt(0)\n",
    "    neg_count = neg_bool.sum(skipna=True)\n",
    "    \n",
    "    total = len(na_bool)\n",
    "    if float(na_count + neg_count) / float(total) > 0.80:\n",
    "        count += 1\n",
    "        toRemove.append(var)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 10595)\n",
      "(4242, 5259)\n"
     ]
    }
   ],
   "source": [
    "#Remove variables that are 80% or more neg / NA (missing/blank) values\n",
    "background_trim_2 = background_trim.copy(deep=True)\n",
    "print(background_trim_2.shape)\n",
    "background_trim_2.drop(toRemove, axis=1, inplace=True)\n",
    "print(background_trim_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 484)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4242 entries, 0 to 4241\n",
      "Columns: 484 entries, challengeID to cf5samp\n",
      "dtypes: float64(93), int64(384), object(7)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Filtering for constructed variables (most of which begin with 'c'):\n",
    "background_c = background_trim_2.filter(regex = '^c', axis = 1)\n",
    "print(background_c.shape)\n",
    "background_c.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  2053128\n",
      "NaN:  10402\n",
      "NaN Fraction: 0.00506641573248\n",
      "Negative:  638907\n",
      "Negative Fraction: 0.311187125206\n"
     ]
    }
   ],
   "source": [
    "#Determine how many na/nan, negative values there are in constructed variable set\n",
    "\n",
    "counts = background_c.count(1).values\n",
    "total = background_c.shape[0]*background_c.shape[1] #background_c.size\n",
    "nan_count = total - sum(counts)\n",
    "\n",
    "print \"Total: \", total\n",
    "print \"NaN: \", nan_count\n",
    "print \"NaN Fraction:\", float(nan_count)/float(total)\n",
    "\n",
    "counts = background_c._get_numeric_data()\n",
    "#print(counts.shape)\n",
    "neg_count = counts.lt(0).sum(skipna=True).sum()\n",
    "print \"Negative: \", neg_count\n",
    "print \"Negative Fraction:\", float(neg_count)/float(total)\n",
    "\n",
    "#stuff = background_c.loc[:,'cf5samp'].values\n",
    "#stuff = [1 for x in stuff if x < 0]\n",
    "#sum(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16782 entries, 0 to 16781\n",
      "Columns: 134 entries, new_name to label58\n",
      "dtypes: float64(59), int64(3), object(72)\n",
      "memory usage: 17.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idnum</td>\n",
       "      <td>ID Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cf1intmon</td>\n",
       "      <td>uc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cf1intyr</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cf1lenhr</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cf1lenmin</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cf1twoc</td>\n",
       "      <td>bin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cf1fint</td>\n",
       "      <td>bin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cf1natsm</td>\n",
       "      <td>bin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>f1natwt</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cf1natsmx</td>\n",
       "      <td>bin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    new_name       type\n",
       "0      idnum  ID Number\n",
       "1  cf1intmon         uc\n",
       "2   cf1intyr       cont\n",
       "3   cf1lenhr       cont\n",
       "4  cf1lenmin       cont\n",
       "5    cf1twoc        bin\n",
       "6    cf1fint        bin\n",
       "7   cf1natsm        bin\n",
       "8    f1natwt       cont\n",
       "9  cf1natsmx        bin"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collect metadata\n",
    "meta_csv.info()\n",
    "meta_csv.axes\n",
    "meta_type = meta_csv.loc[:,['new_name','type']]\n",
    "meta_type.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "#Check if there are any variables in background.csv that are not in metadata\n",
    "meta_dict = {}\n",
    "for i in range(meta_type.shape[0]):\n",
    "    meta_dict[meta_type['new_name'][i]] = meta_type['type'][i]\n",
    "\n",
    "count = 0\n",
    "for var in background_csv.axes[1]:\n",
    "    if var not in meta_dict:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a deep copy of constructed variables df\n",
    "background_c_imputed = background_c.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round0\n",
      "round25\n",
      "round50\n",
      "round75\n",
      "round100\n",
      "round125\n",
      "round150\n",
      "round175\n",
      "round200\n",
      "round225\n",
      "round250\n",
      "round275\n",
      "round300\n",
      "round325\n",
      "round350\n",
      "round375\n",
      "round400\n",
      "round425\n",
      "round450\n",
      "round475\n"
     ]
    }
   ],
   "source": [
    "#Fill in Missing Data in Constructed Variable Set (isna / lt(0))\n",
    "variables = list(background_c_imputed.axes[1][1:])\n",
    "#print(background_c_imputed[variables[0]])\n",
    "\n",
    "toRemove = []\n",
    "for i, var in enumerate(variables):\n",
    "    #if i < 375:\n",
    "    #    continue\n",
    "    if i % 25 == 0:\n",
    "        print \"round\" + str(i)\n",
    "    if var in meta_dict:\n",
    "        if meta_dict[var] == 'bin' or meta_dict[var] == 'uc':\n",
    "            #print \"bin\"\n",
    "            maximum = background_c_imputed.loc[:,var].max()\n",
    "            _mode = background_c_imputed.loc[background_c_imputed.loc[:,var] >= 0,var].mode()[0]\n",
    "            background_c_imputed.loc[:,var].fillna(_mode)\n",
    "            \n",
    "            #Transform Neg Entries to Unordered Categorical Values\n",
    "            for i in range(len(list(background_c_imputed.loc[:,var]))):\n",
    "                if background_c_imputed.loc[i,var] == -7 or background_c_imputed.loc[i,var] == -8:\n",
    "                    background_c_imputed.loc[i,var] = maximum + 1\n",
    "                elif background_c_imputed.loc[i,var] == -6:\n",
    "                    background_c_imputed.loc[i,var] = maximum + 2\n",
    "                elif background_c_imputed.loc[i,var] == -2:\n",
    "                    background_c_imputed.loc[i,var] = maximum + 3\n",
    "                elif background_c_imputed.loc[i,var] == -1:\n",
    "                    background_c_imputed.loc[i,var] = maximum + 4\n",
    "                elif background_c_imputed.loc[i,var] < 0:\n",
    "                    background_c_imputed.loc[i,var] = _mode\n",
    "                    \n",
    "        elif meta_dict[var] == 'string':\n",
    "            #print \"string\"\n",
    "            #DIY Label-Encoding\n",
    "            #print(var)\n",
    "            \n",
    "            #treat as a continuous variable if necessary\n",
    "            #if background_c_imputed.loc[:,var].dtype == float\n",
    "            \n",
    "            unique_values = list(background_c_imputed.loc[:,var].unique())\n",
    "            unique_strings = [x for x in unique_values if type(x) == str or x > 0]\n",
    "            unique_dict = {}\n",
    "            for i, x in enumerate(unique_strings):\n",
    "                unique_dict[x] = i\n",
    "                \n",
    "            #DIY mode\n",
    "            all_values = list(background_c_imputed.loc[:,var].values)\n",
    "            all_strings = [x for x in all_values if type(x) == str or x > 0]\n",
    "            _mode = stats.mode(all_strings)[0][0]\n",
    "                \n",
    "            if len(unique_strings) < 11:\n",
    "                maximum = len(unique_strings) - 1\n",
    "                background_c_imputed.loc[:,var].fillna(_mode)\n",
    "                \n",
    "                #Transform Neg/String Entires to Unordered Categorical Values\n",
    "                for i in range(len(list(background_c_imputed.loc[:,var]))):\n",
    "                    if background_c_imputed.loc[i,var] == -7 or background_c_imputed.loc[i,var] == -8:\n",
    "                        background_c_imputed.loc[i,var] = maximum + 1\n",
    "                    elif background_c_imputed.loc[i,var] == -6:\n",
    "                        background_c_imputed.loc[i,var] = maximum + 2\n",
    "                    elif background_c_imputed.loc[i,var] == -2:\n",
    "                        background_c_imputed.loc[i,var] = maximum + 3\n",
    "                    elif background_c_imputed.loc[i,var] == -1:\n",
    "                        background_c_imputed.loc[i,var] = maximum + 4\n",
    "                    elif background_c_imputed.loc[i,var] < 0:\n",
    "                        background_c_imputed.loc[i,var] = unique_dict[_mode]\n",
    "                    else:\n",
    "                        background_c_imputed.loc[i,var] = unique_dict[background_c_imputed.loc[i,var]]\n",
    "                \n",
    "            else:\n",
    "                toRemove.append(var)            \n",
    "            \n",
    "        elif meta_dict[var] == 'oc':\n",
    "            #print \"oc\"\n",
    "            _mode = background_c_imputed.loc[(background_c_imputed.loc[:,var] >= 0) & (background_c_imputed.loc[:,var] < 200), var].mode()[0]\n",
    "            background_c_imputed.loc[:,var].fillna(_mode)\n",
    "            \n",
    "            #Transform Neg Entries to Mode Ordered Categorical Value\n",
    "            for i in range(len(background_c_imputed[var])):\n",
    "                if background_c_imputed.loc[i,var] < 0 or background_c_imputed.loc[i,var] >= 200:\n",
    "                    background_c_imputed.loc[i,var] = _mode\n",
    "            \n",
    "            \n",
    "        elif meta_dict[var] == 'cont':\n",
    "            #print \"cont\"\n",
    "            _mean = background_c_imputed.loc[background_c_imputed.loc[:,var] >= 0, var].mean()\n",
    "            background_c_imputed.loc[:,var].fillna(_mean)\n",
    "            \n",
    "            #Transform Neg Entries to Mean Continuous Value\n",
    "            for i in range(len(list(background_c_imputed.loc[:,var]))):\n",
    "                if background_c_imputed.loc[i,var] < 0:\n",
    "                    background_c_imputed.loc[i,var] = _mean\n",
    "            \n",
    "        else:\n",
    "            print(meta_dict[var])\n",
    "    else:\n",
    "        #cannot discern type, set to mode\n",
    "        print \"nope\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print out a brief subset of constructed variable data before imputation, and afterwards\n",
    "to get a sense of how the data gets cleaner with respect to different varaiable data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cont', 'cont', 'bin', 'bin', 'cont', 'bin', 'bin', 'cont', 'cont', 'bin']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cf1lenhr</th>\n",
       "      <th>cf1lenmin</th>\n",
       "      <th>cf1fint</th>\n",
       "      <th>cf1citsm</th>\n",
       "      <th>cf1age</th>\n",
       "      <th>cf1marm</th>\n",
       "      <th>cf1cohm</th>\n",
       "      <th>cf1adult</th>\n",
       "      <th>cf1kids</th>\n",
       "      <th>cf1gdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cf1lenhr  cf1lenmin  cf1fint  cf1citsm  cf1age  cf1marm  cf1cohm  \\\n",
       "0         -9         -9        0        -9      -9       -9       -9   \n",
       "1          0         40        1         1      -3        0        0   \n",
       "2          0         45        1         1      24        1        0   \n",
       "3          0         45        1         1      24        0        1   \n",
       "4         -6         50        1         1      19        0        0   \n",
       "5          0         30        1         1      20        0        1   \n",
       "6          0         45        1         1      -3        0        1   \n",
       "7         -3         45        1         1      26        0        0   \n",
       "8         -3         55        1         1      24        1        0   \n",
       "9          0         45        1         1      34        0        1   \n",
       "10         0         30        1         1      -3        0        0   \n",
       "\n",
       "    cf1adult  cf1kids  cf1gdad  \n",
       "0         -9       -9       -9  \n",
       "1          1        0        0  \n",
       "2          5        1        1  \n",
       "3          2        0        0  \n",
       "4          5        0        0  \n",
       "5          4        2        1  \n",
       "6          3        0        0  \n",
       "7          4        0        1  \n",
       "8          2        1        0  \n",
       "9          2        0        0  \n",
       "10         1        0        0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = [meta_dict[var] for var in variables[0:10]]\n",
    "print meta \n",
    "background_c.loc[0:10,variables[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cf1lenhr</th>\n",
       "      <th>cf1lenmin</th>\n",
       "      <th>cf1fint</th>\n",
       "      <th>cf1citsm</th>\n",
       "      <th>cf1age</th>\n",
       "      <th>cf1marm</th>\n",
       "      <th>cf1cohm</th>\n",
       "      <th>cf1adult</th>\n",
       "      <th>cf1kids</th>\n",
       "      <th>cf1gdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178152</td>\n",
       "      <td>36.753973</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.92677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.404432</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.92677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178152</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.92677</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.178152</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.178152</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.92677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cf1lenhr  cf1lenmin  cf1fint  cf1citsm    cf1age  cf1marm  cf1cohm  \\\n",
       "0   0.178152  36.753973        0         1  27.92677        0        0   \n",
       "1   0.000000  40.000000        1         1  27.92677        0        0   \n",
       "2   0.000000  45.000000        1         1  24.00000        1        0   \n",
       "3   0.000000  45.000000        1         1  24.00000        0        1   \n",
       "4   0.178152  50.000000        1         1  19.00000        0        0   \n",
       "5   0.000000  30.000000        1         1  20.00000        0        1   \n",
       "6   0.000000  45.000000        1         1  27.92677        0        1   \n",
       "7   0.178152  45.000000        1         1  26.00000        0        0   \n",
       "8   0.178152  55.000000        1         1  24.00000        1        0   \n",
       "9   0.000000  45.000000        1         1  34.00000        0        1   \n",
       "10  0.000000  30.000000        1         1  27.92677        0        0   \n",
       "\n",
       "    cf1adult   cf1kids  cf1gdad  \n",
       "0   2.404432  0.956615        0  \n",
       "1   1.000000  0.000000        0  \n",
       "2   5.000000  1.000000        1  \n",
       "3   2.000000  0.000000        0  \n",
       "4   5.000000  0.000000        0  \n",
       "5   4.000000  2.000000        1  \n",
       "6   3.000000  0.000000        0  \n",
       "7   4.000000  0.000000        1  \n",
       "8   2.000000  1.000000        0  \n",
       "9   2.000000  0.000000        0  \n",
       "10  1.000000  0.000000        0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_c_imputed.loc[0:10,variables[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ch5dspr',\n",
       " 'ch5ppvtae',\n",
       " 'ch5ppvtpr',\n",
       " 'ch5wj9pr',\n",
       " 'ch5wj9ae',\n",
       " 'ch5wj10pr',\n",
       " 'ch5wj10ae',\n",
       " 'ch5dsae']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toRemove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch5wj9pr\n",
      "ch5dspr\n",
      "ch5wj10pr\n",
      "ch5ppvtpr\n"
     ]
    }
   ],
   "source": [
    "#these guys are potentially annoying, they should be labelled continuous but they're strings\n",
    "#it is difficult to determine if any particular row has this awkward labeling so let's just throw them out  \n",
    "pattern = re.compile(\".*pr$\")\n",
    "for key in meta_dict:\n",
    "    if pattern.match(key):\n",
    "        print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 484)\n",
      "(4242, 476)\n"
     ]
    }
   ],
   "source": [
    "print(background_c_imputed.shape)\n",
    "background_c_imputed.drop(toRemove, axis=1, inplace=True)\n",
    "print(background_c_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "262\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "#extract unordered categorical variables (uc, bin, string) and cont/oc vars (euclidean)\n",
    "\n",
    "variables = list(background_c_imputed.axes[1][1:])\n",
    "uc_columns = [var for var in variables if meta_dict[var] in ['uc','bin','string']]\n",
    "other_columns = [var for var in variables if meta_dict[var] in ['cont','oc']]\n",
    "print(len(variables))\n",
    "print(len(uc_columns))\n",
    "print(len(other_columns))\n",
    "\n",
    "#Note we get rid of the challengeID column here as a result of excluding it from variabes \n",
    "#list we gerenated previously.  Therefore we go from 476 variables above to 475 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 262)\n",
      "(4242, 213)\n"
     ]
    }
   ],
   "source": [
    "uc_df = background_c_imputed.filter(items=uc_columns)\n",
    "print(uc_df.shape)\n",
    "other_df = background_c_imputed.filter(items=other_columns)\n",
    "print(other_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cf1fint', 'cf1citsm', 'cf1marm', 'cf1cohm', 'cf1gdad', 'cf1gmom', 'cf1ethrace', 'cf1hhimp', 'cf1finjail', 'cf1tele']\n",
      "['cf1fint', 'cf1citsm', 'cf1marm', 'cf1cohm', 'cf1gdad', 'cf1gmom', 'cf1ethrace', 'cf1hhimp', 'cf1finjail', 'cf1tele']\n"
     ]
    }
   ],
   "source": [
    "print list(uc_df.axes[1][0:10])\n",
    "print uc_columns[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at distribution of spread and magnitude for each cont variable\n",
    "spread = []\n",
    "mean = []\n",
    "minimum = []\n",
    "maximum = []\n",
    "sd = []\n",
    "for var in other_df.axes[1]:\n",
    "    spread.append(other_df.loc[:,var].max() - other_df.loc[:,var].min())\n",
    "    mean.append(other_df.loc[:,var].mean())\n",
    "    minimum.append(other_df.loc[:,var].min())\n",
    "    maximum.append(other_df.loc[:,var].max())\n",
    "    sd.append(other_df.loc[:,var].std())\n",
    "    \n",
    "#plt.plot([i for i in range(len(spread))],spread, 'ro')\n",
    "#plt.plot([i for i in range(len(minimum))],minimum, 'bo')\n",
    "#plt.semilogy([i for i in range(len(maximum))],maximum, 'go')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Min Scale cont/oc data to [0,1] range for better coeff interpretability\n",
    "other_mat = other_df.as_matrix()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "other_out = min_max_scaler.fit_transform(other_mat)\n",
    "\n",
    "#Collect UC data, no re-scaling necessary\n",
    "uc_mat = uc_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642\n",
      "642\n",
      "(4242, 642)\n"
     ]
    }
   ],
   "source": [
    "#Perform one-hot encoding on uc_mat to make uc data type compatible with machine learning classifiers\n",
    "#will increase number of \"features\" by some small factor, but worth it for semantic reasons\n",
    "#IMPORTANT: ASSUMES EACH FEATURE TAKES ON VALUES IN RANGE(0, n_values), SO WHILE MOST OF\n",
    "#OUR CATEGORICAL VARIABLES DO ADHERE TO THIS SOME DONT START AT 0, RESULT IS A FEW MORE\n",
    "#\n",
    "\n",
    "print(uc_df.nunique().sum()) #should be resulting size of one hot matrix\n",
    "\n",
    "#Also I need to somehow map indexes in expanded one-hot form back to original\n",
    "#uc_columns index to associate variables with important \"one-hot features\"\n",
    "indexToVar = []\n",
    "for i, var in enumerate(uc_columns):\n",
    "    for j in range(uc_df[var].nunique()):\n",
    "        indexToVar.append(i)\n",
    "\n",
    "print len(indexToVar)\n",
    "        \n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(uc_mat)\n",
    "uc_out = enc.transform(uc_mat)\n",
    "print uc_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 855)\n"
     ]
    }
   ],
   "source": [
    "#Combine both types of data\n",
    "new_matrix = []\n",
    "\n",
    "for i in range(other_out.shape[0]):\n",
    "    temp = []\n",
    "    for j in range(uc_out.shape[1]):\n",
    "        temp.append(uc_out[i,j])\n",
    "    for j in range(other_out.shape[1]):\n",
    "        temp.append(other_out[i,j])\n",
    "    new_matrix.append(temp)\n",
    "    \n",
    "new_matrix = np.matrix(new_matrix)\n",
    "print new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466\n",
      "(4242, 855)\n"
     ]
    }
   ],
   "source": [
    "#Separate Training and Testing Data\n",
    "\n",
    "#Dropping rows with only NAs in training data\n",
    "train_noNARows = train_csv.dropna(thresh=2)\n",
    "#Grabbing the unique IDs of participants in the training set:\n",
    "train_IDs = list(train_noNARows.iloc[:,0])\n",
    "\n",
    "#I'm pretty sure that the range index for this data goes from 1 to 4242 as does 'challengeID'\n",
    "#However, since we have converted everything to a matrix, we are now indexing\n",
    "#From 0 to 4241 in the training data\n",
    "train_IDs = [x - 1 for x in train_IDs] \n",
    "print len(train_IDs)\n",
    "\n",
    "#Grab rows in processed data corresponding to unique IDs of valid training labels\n",
    "train_data = new_matrix[train_IDs]\n",
    "\n",
    "#Get all testing data\n",
    "test_data = new_matrix\n",
    "print test_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 9, 10, 13, 14, 16, 18, 20, 23]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_noNARows.iloc[:,0])[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>challengeID</th>\n",
       "      <th>gpa</th>\n",
       "      <th>grit</th>\n",
       "      <th>materialHardship</th>\n",
       "      <th>eviction</th>\n",
       "      <th>layoff</th>\n",
       "      <th>jobTraining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>2.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    challengeID   gpa  grit  materialHardship  eviction  layoff  jobTraining\n",
       "2             6   NaN  3.50          0.090909       0.0     0.0          0.0\n",
       "3             7  2.50  3.25          0.000000       0.0     0.0          0.0\n",
       "5             9  2.25  4.00          0.181818       0.0     0.0          0.0\n",
       "6            10  3.25  3.25          0.090909       0.0     NaN          0.0\n",
       "7            13  2.75  4.00          0.181818       0.0     0.0          1.0\n",
       "8            14  3.25  2.75          0.272727       0.0     1.0          0.0\n",
       "9            16  2.00  3.50          0.090909       0.0     0.0          1.0\n",
       "10           18  2.25  3.00          0.000000       0.0     1.0          0.0\n",
       "11           20   NaN  3.00          0.000000       0.0     0.0          0.0\n",
       "12           23  2.50  3.25          0.000000       0.0     0.0          0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_noNARows.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06332423829013187"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute average NA proportion of training labels\n",
    "\n",
    "count = 0.0\n",
    "for i in range(6):\n",
    "    count += float(train_noNARows.shape[0] - train_noNARows.iloc[:,i+1].count()) / float(train_noNARows.shape[0])\n",
    "count / float(6)\n",
    "\n",
    "#Even though 6% is tolerable, we should not run label vector with NaN inside our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layoff': 0, 'grit': 3.4275387870239773, 'materialHardship': 0.10374478160633066, 'jobTraining': 0, 'gpa': 2.8667381974248927, 'challengeID': 0, 'eviction': 0}\n"
     ]
    }
   ],
   "source": [
    "#We will impute training labels with basic mean (cont outcomes) and mode (binary outcomes)\n",
    "#We might come back to this and do sophisticated KNN based imputation but \n",
    "#It is really not worth the work\n",
    "\n",
    "col_labels=['challengeID','gpa','grit','materialHardship','eviction','layoff','jobTraining']\n",
    "cont_labels = ['gpa','grit','materialHardship']\n",
    "bin_labels = ['eviction','layoff','jobTraining']\n",
    "\n",
    "values = {}\n",
    "\n",
    "for label in col_labels:\n",
    "    if label in cont_labels:\n",
    "        values[label] = train_noNARows.loc[:,label].mean()\n",
    "    elif label in bin_labels:\n",
    "        values[label] = int(train_noNARows.loc[:,label].mode())\n",
    "    else:\n",
    "        values[label] = 0\n",
    "\n",
    "print values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>challengeID</th>\n",
       "      <th>gpa</th>\n",
       "      <th>grit</th>\n",
       "      <th>materialHardship</th>\n",
       "      <th>eviction</th>\n",
       "      <th>layoff</th>\n",
       "      <th>jobTraining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2.866738</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>2.866738</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    challengeID       gpa  grit  materialHardship  eviction  layoff  \\\n",
       "2             6  2.866738  3.50          0.090909       0.0     0.0   \n",
       "3             7  2.500000  3.25          0.000000       0.0     0.0   \n",
       "5             9  2.250000  4.00          0.181818       0.0     0.0   \n",
       "6            10  3.250000  3.25          0.090909       0.0     0.0   \n",
       "7            13  2.750000  4.00          0.181818       0.0     0.0   \n",
       "8            14  3.250000  2.75          0.272727       0.0     1.0   \n",
       "9            16  2.000000  3.50          0.090909       0.0     0.0   \n",
       "10           18  2.250000  3.00          0.000000       0.0     1.0   \n",
       "11           20  2.866738  3.00          0.000000       0.0     0.0   \n",
       "12           23  2.500000  3.25          0.000000       0.0     0.0   \n",
       "\n",
       "    jobTraining  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "5           0.0  \n",
       "6           0.0  \n",
       "7           1.0  \n",
       "8           0.0  \n",
       "9           1.0  \n",
       "10          0.0  \n",
       "11          0.0  \n",
       "12          0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_noNARows = train_noNARows.fillna(value=values)\n",
    "train_noNARows.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and Separate labels accordingly\n",
    "\n",
    "train_labels = {}\n",
    "train_labels['gpa'] = train_noNARows['gpa'].as_matrix()\n",
    "train_labels['grit'] = train_noNARows['grit'].as_matrix()\n",
    "train_labels['materialHardship'] = train_noNARows['materialHardship'].as_matrix()\n",
    "train_labels['eviction']= train_noNARows['eviction'].as_matrix()\n",
    "train_labels['layoff'] = train_noNARows['layoff'].as_matrix()\n",
    "train_labels['jobTraining'] = train_noNARows['jobTraining'].as_matrix()\n",
    "\n",
    "#print(gpa_label[0:10])\n",
    "#print(len(gpa_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059345156889495224\n",
      "0.18212824010914053\n",
      "0.2339699863574352\n"
     ]
    }
   ],
   "source": [
    "#It is useful to see how skewed the binary labels are (most of the labels are 0.0)\n",
    "#so we should subsample our training data accordingly for a more balanced model\n",
    "\n",
    "print(np.mean(train_labels['eviction']))\n",
    "print(np.mean(train_labels['layoff']))\n",
    "print(np.mean(train_labels['jobTraining']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Let's see what testing on only one hot or only cont data is like\n",
    "# #cont_data = other_out[train_IDs]\n",
    "# #hot_data = uc_out[train_IDs]\n",
    "\n",
    "# #Let's see what balancing out binary labels can do for logistic regression\n",
    "# def balance_data(label,factor):\n",
    "    \n",
    "#     #extract positive vs negative label IDs\n",
    "#     train_IDs_1 = train_noNARows.loc[train_noNARows[label] == 1.0,'challengeID']\n",
    "#     train_IDs_1 = [x - 1 for x in train_IDs_1] \n",
    "#     train_IDs_0 = train_noNARows.loc[train_noNARows[label] == 0.0,'challengeID']\n",
    "#     train_IDs_0 = [x - 1 for x in train_IDs_0] \n",
    "\n",
    "#     balanced_IDs = list(train_IDs_1)\n",
    "#     balanced_IDs.extend(train_IDs_0[0:int(factor*len(train_IDs_1))])\n",
    "#     balanced_IDs.sort()\n",
    "#     balanced_data = new_matrix[balanced_IDs]\n",
    "#     balanced_IDs = [x + 1 for x in balanced_IDs]\n",
    "#     balanced_label = train_noNARows.loc[train_noNARows['challengeID'].isin(balanced_IDs),label].as_matrix()\n",
    "#     return balanced_data, balanced_label\n",
    "\n",
    "# #confirm we pulled right amount out\n",
    "# #print(len(balanced_IDs)) #2.5*len(train_IDs_1)\n",
    "# #print(balanced_job.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logistic_regression(label, factor, regularizer, solver_):\n",
    "#     balanced_data, balanced_label = balance_data(label,factor)\n",
    "#     clf = linear_model.LogisticRegressionCV(cv=5, penalty=regularizer, solver=solver_)\n",
    "#     clf.fit(balanced_data,balanced_label)\n",
    "#     print(\"Logistic Regression with l2 regularization on {}\".format(label))\n",
    "#     print(\"Proportion of 1 (Positive) Labels: {}\".format(np.mean(balanced_label)))\n",
    "#     print(\"Brier Loss Score: {}\".format(metrics.brier_score_loss(train_labels[label], prediction[:,1])))\n",
    "\n",
    "# #Best label proportion for job prediction is 40% i.e. 1.5*train_IDs_1\n",
    "# clf = logistic_regression('jobTraining','l2','lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purpose of Dual and Intercept_Scaling Parameters\n",
    "#dual is only used for l2 regularization with liblinear solver\n",
    "#intercept_scaling useful only when liblinear solver is used\n",
    "\n",
    "#IMPORTANT BY SELECTING class_weight='balanced' I am forcing LogRegCV to correct\n",
    "#Weights via inverse scaling wrt class frequency - aka I am removing huge 0-label bias in the data\n",
    "#This means my K-Fold CV Brier Loss will look weaker on my training set, but as long as the \n",
    "#leaderboard / held-out test samples are more balanced, I foresee stronger classification \n",
    "\n",
    "def feedToBrier(y_true, y_prob):\n",
    "    return metrics.brier_score_loss(y_true, y_prob[:,1])\n",
    "\n",
    "scorer_ = metrics.make_scorer(feedToBrier, greater_is_better=False,needs_proba=True)\n",
    "\n",
    "def logistic_regression(label, regularizer, solver_):\n",
    "    clf = linear_model.LogisticRegressionCV(Cs=10, fit_intercept=True, cv=5, dual=False, penalty=regularizer, \n",
    "                                            scoring=scorer_, solver=solver_, max_iter=100, tol=0.0001, class_weight='balanced', \n",
    "                                            n_jobs=1, verbose=0, refit=True, intercept_scaling=1.0, multi_class='ovr', \n",
    "                                            random_state=None)\n",
    "    clf.fit(train_data,train_labels[label])\n",
    "    return clf\n",
    "\n",
    "# Purpose of Refit Parameter very Important\n",
    "# If set to True, the scores are averaged across all folds, and \n",
    "# the coefs and the C that corresponds to the best score is taken, \n",
    "# and a final refit is done using these parameters. Otherwise the coefs, \n",
    "# intercepts and C that correspond to the best scores across folds are averaged.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logistic(clf, label, regularizer):\n",
    "    print(\"Logistic Regression with {} regularization on {} \\n\".format(regularizer, label))\n",
    "    print \"Regularization Constants\"\n",
    "    print clf.Cs_, \"\\n\"\n",
    "    print \"Average Brier Loss per Regularization Constant Over All Folds\"\n",
    "    scores = []\n",
    "    for i in range(len(clf.scores_[1.0][0])):\n",
    "        scores.append(-1.0*max(clf.scores_[1.0][:,i]))\n",
    "    print scores, \"\\n\"\n",
    "    print \"Smallest Brier Loss Over All Folds and Choices of C\"\n",
    "    print np.min(scores), \"\\n\"\n",
    "    #print clf_job_l2.scores_\n",
    "    print \"Most Informative Features\"\n",
    "    informative = np.argsort(clf.coef_[0])\n",
    "    print informative[-10:], \"\\n\"\n",
    "    print \"Best Score Across Every Class\"\n",
    "    print clf.C_, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with Built In Cross Validation to Select the Best Regularization Weight / Coefficients with respect to Brier Loss Metric\n",
    "\n",
    "clf_job_l2 = logistic_regression('jobTraining','l2','lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with l2 regularization on jobTraining \n",
      "\n",
      "Regularization Constants\n",
      "[1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04] \n",
      "\n",
      "Average Brier Loss per Regularization Constant Over All Folds\n",
      "[0.2489887958606962, 0.24483242191946783, 0.23574889776924607, 0.23322363013463204, 0.24501988053299772, 0.26864954799413165, 0.29976206244822967, 0.3152329881716158, 0.31545167166225463, 0.3163462949408531] \n",
      "\n",
      "Smallest Brier Loss Over All Folds and Choices of C\n",
      "0.23322363013463204 \n",
      "\n",
      "Most Informative Features\n",
      "[849 416 576  81  49 852 343 657  13 844] \n",
      "\n",
      "Best Score Across Every Class\n",
      "[0.00599484] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cs array = inverse of regularization strength\n",
    "#Smaller C tended to perform better, aka, stronger regularization has better KFold_CV performance\n",
    "#This is not surprising\n",
    "print_logistic(clf_job_l2,'jobTraining','l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with Built In Cross Validation to Select the Best Regularization Weight / Coefficients with respect to Brier Loss Metric\n",
    "\n",
    "clf_job_l1 = logistic_regression('jobTraining','l1','saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with l1 regularization on jobTraining \n",
      "\n",
      "Regularization Constants\n",
      "[1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04] \n",
      "\n",
      "Average Brier Loss per Regularization Constant Over All Folds\n",
      "[0.24334496852377088, 0.2429775791843483, 0.2433313830559145, 0.24216966591364272, 0.23604126884997162, 0.260388988831333, 0.2706073259208015, 0.27911259535760036, 0.2847666200666093, 0.2884602484259553] \n",
      "\n",
      "Smallest Brier Loss Over All Folds and Choices of C\n",
      "0.23604126884997162 \n",
      "\n",
      "Most Informative Features\n",
      "[445 176 841 160 343 852  13 814 844 797] \n",
      "\n",
      "Best Score Across Every Class\n",
      "[0.35938137] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#L1 performance is comparable with L2, it is a stronger suppressor on feature weights\n",
    "#So it has more consistent performance independent of size of tuning parameter C\n",
    "#I suspect, because of the stronger suppression, the most important features for L1 hold a lot of weight\n",
    "\n",
    "print_logistic(clf_job_l1,'jobTraining','l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with Built In Cross Validation to Select the Best Regularization Weight / Coefficients with respect to Brier Loss Metric\n",
    "\n",
    "clf_evict_l2 = logistic_regression('eviction','l2','lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with l2 regularization on eviction \n",
      "\n",
      "Regularization Constants\n",
      "[1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04] \n",
      "\n",
      "Average Brier Loss per Regularization Constant Over All Folds\n",
      "[0.2430910979029625, 0.22398284347848366, 0.18025506807649477, 0.13906519730674904, 0.12093011656108776, 0.11140713988924242, 0.10988015855989364, 0.11211314835231886, 0.11439616189325086, 0.11660255341528108] \n",
      "\n",
      "Smallest Brier Loss Over All Folds and Choices of C\n",
      "0.10988015855989364 \n",
      "\n",
      "Most Informative Features\n",
      "[722 407 637 764 674 804 805 329 461 476] \n",
      "\n",
      "Best Score Across Every Class\n",
      "[2.7825594] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_logistic(clf_evict_l2,'eviction','l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with Built In Cross Validation to Select the Best Regularization Weight / Coefficients with respect to Brier Loss Metric\n",
    "\n",
    "clf_evict_l1 = logistic_regression('layoff','l1','saga')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with l1 regularization on evict \n",
      "\n",
      "Regularization Constants\n",
      "[1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04] \n",
      "\n",
      "Average Brier Loss per Regularization Constant Over All Folds\n",
      "[0.23620668647166956, 0.2404228245097979, 0.2448083859462886, 0.24504396435429357, 0.23553579207940767, 0.2440468425568228, 0.2523364804601218, 0.2576090641235622, 0.26101686301991345, 0.26379461580524155] \n",
      "\n",
      "Smallest Brier Loss Over All Folds and Choices of C\n",
      "0.23553579207940767 \n",
      "\n",
      "Most Informative Features\n",
      "[569 382 743  13 264  65 845 176 637 686] \n",
      "\n",
      "Best Score Across Every Class\n",
      "[0.35938137] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_logistic(clf_evict_l1,'evict','l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with Built In Cross Validation to Select the Best Regularization Weight / Coefficients with respect to Brier Loss Metric\n",
    "\n",
    "clf_layoff_l2 = logistic_regression('layoff','l2','lbfgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with l2 regularization on layoff \n",
      "\n",
      "Regularization Constants\n",
      "[1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04] \n",
      "\n",
      "Average Brier Loss per Regularization Constant Over All Folds\n",
      "[0.24810942608069655, 0.24423322861418692, 0.2398930384295608, 0.23079195590981844, 0.23187258622314977, 0.2499663359085006, 0.27308770558748763, 0.29037177155851873, 0.285795757015524, 0.2919010226203595] \n",
      "\n",
      "Smallest Brier Loss Over All Folds and Choices of C\n",
      "0.23079195590981844 \n",
      "\n",
      "Most Informative Features\n",
      "[359 563 164 277 264 569 591  11  49  13] \n",
      "\n",
      "Best Score Across Every Class\n",
      "[0.00599484] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_logistic(clf_layoff_l2,'layoff','l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with Built In Cross Validation to Select the Best Regularization Weight / Coefficients with respect to Brier Loss Metric\n",
    "\n",
    "clf_layoff_l1 = logistic_regression('layoff','l1','saga')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with l1 regularization on layoff \n",
      "\n",
      "Regularization Constants\n",
      "[1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04] \n",
      "\n",
      "Average Brier Loss per Regularization Constant Over All Folds\n",
      "[0.24617614138501256, 0.23902809174893483, 0.24131248843359945, 0.24504392998291738, 0.2354979974570491, 0.24417006931989832, 0.25217996806793974, 0.2577167313870195, 0.2610373220343355, 0.26377841318692874] \n",
      "\n",
      "Smallest Brier Loss Over All Folds and Choices of C\n",
      "0.2354979974570491 \n",
      "\n",
      "Most Informative Features\n",
      "[283 284 285 286 287 288 289 290 292 854] \n",
      "\n",
      "Best Score Across Every Class\n",
      "[0.00599484] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_logistic(clf_layoff_l1,'layoff','l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = linear_model.LinearRegression()\n",
    "# clf = linear_model.ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],cv=5,max_iter=1000)\n",
    "# clf.fit(cont_data,gpa_label)\n",
    "# prediction = clf.predict(cont_data)\n",
    "# print(prediction[0:10])\n",
    "# print(\"Mean Squared Error:{}\".format(metrics.mean_squared_error(gpa_label, prediction)))\n",
    "# print(\"Chosen Alpha:{}\".format(cld.alpha_))\n",
    "# print(\"Chosen l1 Ratio:{}\".format(clf.l1_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE THAT WE SET FIT_INTERCEPT TO TRUE EVEN THOUGH WE ARE USING ONE HOT ENCODING WITH COLINEARITY \n",
    "#BECUASE REGULARIZATION DEALS WITH THIS PROLBLEM\n",
    "\n",
    "#ANOTHER IMPORTANT PARAMETER IS ALPHA - THIS IS THE AMOUNT OF PENALIZATION CHOSEN BY CV\n",
    "#L!_RATIO IS THE COMPROMIZE BETWEEN L1 and L2 PENALIZATION CHOSEN BY CV\n",
    "#SELECTION='random' ALLOWS RANDOM COEFF TO BE UPDATED AT EACH STEP, YIELDS SIGNIFICANT CONVERGENCE SPEED UP\n",
    "\n",
    "scorer_ = metrics.make_scorer(metrics.mean_squared_error, greater_is_better=False)\n",
    "\n",
    "def linear_regression(label, regularizer, selection_):\n",
    "    clf = linear_model.ElasticNetCV(l1_ratio=regularizer, eps=0.001, n_alphas=100, alphas=None, \n",
    "                                    fit_intercept=True, normalize=False, precompute='auto', \n",
    "                                    max_iter=1000, tol=0.0001, cv=5, copy_X=True, verbose=0, \n",
    "                                    n_jobs=1, positive=False, random_state=None, selection=selection_)\n",
    "    clf.fit(train_data,train_labels[label])\n",
    "    return clf\n",
    "\n",
    "def linear_regression_post(label, regularizer, selection_, alphas_):\n",
    "    clf = linear_model.ElasticNetCV(l1_ratio=regularizer, eps=0.001, alphas=alphas_, \n",
    "                                    fit_intercept=True, normalize=False, precompute='auto', \n",
    "                                    max_iter=1000, tol=0.0001, cv=5, copy_X=True, verbose=0, \n",
    "                                    n_jobs=1, positive=False, random_state=None, selection=selection_)\n",
    "    clf.fit(train_data,train_labels[label])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_linear(clf, label, selection):\n",
    "    print(\"Linear Regression with Elastic Regularization and '{}' Selection on {}\\n\".format(selection, label))\n",
    "    print \"Optimal L1_Ratio Chosen (higher means more L1)\"\n",
    "    print clf.l1_ratio_, \"\\n\"\n",
    "    print \"Optimal Alpha Chosen\"\n",
    "    print clf.alpha_, \"\\n\"\n",
    "    ##print \"Grid of Alphas For Each L1 Ratio\"\n",
    "    ##print clf.alphas_, \"\\n\"\n",
    "    #print \"Smallest MSE With Respect to each L1 Ratio\"\n",
    "    #print [np.min(x) for x in clf.mse_path_], \"\\n\"\n",
    "    #print \"Smallest MSE With Respect to Each Fold\"\n",
    "    #print [np.min(clf.mse_path_[:,:,i]) for i in range(0,5)], \"\\n\"\n",
    "    print \"Smallest MSE Overall\"\n",
    "    print np.min(clf.mse_path_), \"\\n\"\n",
    "    print \"Largest MSE Overall\"\n",
    "    print np.max(clf.mse_path_), \"\\n\"\n",
    "    #print clf_job_l2.scores_\n",
    "    print \"Most Informative Features\"\n",
    "    informative = np.argsort(clf.coef_)\n",
    "    print informative[-10:], \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_gpa_linear = linear_regression('gpa',[.1, .5, .7, .9, .95, .99, 1],'random')\n",
    "clf_gpa_linear = linear_regression_post('gpa',1.0,'random',[0.0077890781657518095])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(855,)\n"
     ]
    }
   ],
   "source": [
    "print(clf_gpa_linear.mse_path_.shape)\n",
    "print(clf_gpa_linear.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression with Elastic Regularization and 'random' Selection on gpa\n",
      "\n",
      "Optimal L1_Ratio Chosen (higher means more L1)\n",
      "1.0 \n",
      "\n",
      "Optimal Alpha Chosen\n",
      "0.0077890781657518095 \n",
      "\n",
      "Smallest MSE Overall\n",
      "0.26918283592103476 \n",
      "\n",
      "Largest MSE Overall\n",
      "0.38423223422546465 \n",
      "\n",
      "Most Informative Features\n",
      "[851 430 534  34 154  16 682 657 647 816] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_linear(clf_gpa_linear,'gpa','random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_grit_linear = linear_regression('grit',[.1, .5, .7, .9, .95, .99, 1],'random')\n",
    "clf_grit_linear = linear_regression_post('grit',0.1,'random',[0.07281736504717284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression with Elastic Regularization and 'random' Selection on grit\n",
      "\n",
      "Optimal L1_Ratio Chosen (higher means more L1)\n",
      "0.1 \n",
      "\n",
      "Optimal Alpha Chosen\n",
      "0.07281736504717284 \n",
      "\n",
      "Smallest MSE Overall\n",
      "0.21311509504225082 \n",
      "\n",
      "Largest MSE Overall\n",
      "0.22897403014620205 \n",
      "\n",
      "Most Informative Features\n",
      "[410 233 383  55 230 154 254  49  33 593] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_linear(clf_grit_linear,'grit','random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_hard_linear = linear_regression('materialHardship',[.1, .5, .7, .9, .95, .99, 1],'random')\n",
    "clf_hard_linear = linear_regression_post('materialHardship',0.1,'random',[0.01951356598623485])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression with Elastic Regularization and 'random' Selection on materialHardship\n",
      "\n",
      "Optimal L1_Ratio Chosen (higher means more L1)\n",
      "0.1 \n",
      "\n",
      "Optimal Alpha Chosen\n",
      "0.01951356598623485 \n",
      "\n",
      "Smallest MSE Overall\n",
      "0.019327990102053348 \n",
      "\n",
      "Largest MSE Overall\n",
      "0.025832091762808106 \n",
      "\n",
      "Most Informative Features\n",
      "[ 81 255  45 416 673 844 384 594  13 406] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_linear(clf_hard_linear,'materialHardship','random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_csv = pd.read_csv(pred_path, low_memory=False)\n",
    "# prediction_proba = clf.predict_proba(test_data)\n",
    "# prediction = clf.predict(test_data)\n",
    "\n",
    "# #data= pd.DataFrame({'jobTraining': prediction_proba[:,1]})\n",
    "# prediction_csv.drop(['jobTraining'], axis=1, inplace=True)\n",
    "# prediction_csv['jobTraining'] = pd.Series(prediction_proba[:,1], index=prediction_csv.index)\n",
    "\n",
    "# prediction_csv.to_csv(\"output_prediction.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_prediction = clf_job_l2.predict_proba(test_data)\n",
    "evict_prediction = clf_evict_l2.predict_proba(test_data)\n",
    "layoff_prediction = clf_layoff_l2.predict_proba(test_data)\n",
    "gpa_prediction = clf_gpa_linear.predict(test_data)\n",
    "grit_prediction = clf_grit_linear.predict(test_data)\n",
    "hard_prediction = clf_hard_linear.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'challengeID' : [i for i in range(1,4243)], 'gpa' : gpa_prediction, 'grit': grit_prediction, \n",
    "     'materialHardship': hard_prediction, 'eviction': evict_prediction[:,1], 'layoff': layoff_prediction[:,1], \n",
    "     'jobTraining': job_prediction[:,1]}\n",
    "output_prediction = pd.DataFrame(data=d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prediction.to_csv(\"output_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scorer_ = metrics.make_scorer(metrics.mean_squared_error, greater_is_better=False)\n",
    "\n",
    "#scorer_ = metrics.make_scorer(feedToBrier, greater_is_better=False,needs_proba=True)\n",
    "\n",
    "scorer_ = metrics.make_scorer(metrics.accuracy_score)\n",
    "\n",
    "def get_SVC(C_,kernel_): #C=1.0, kernel='rbf'\n",
    "    return svm.SVC(C=C_, kernel=kernel_, degree=3, gamma='auto', coef0=0.0, \n",
    "                    shrinking=True, probability=False, tol=0.001, cache_size=500, \n",
    "                    class_weight='balanced', verbose=False, max_iter=-1, \n",
    "                    decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "def gridsearchSVC(label):\n",
    "    params = {\"kernel\":[ \"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "           \"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    grid = GridSearchCV(get_SVC(1.0,'rbf'), params, cv=5, scoring=scorer_, refit=True, iid=True)\n",
    "    grid.fit(train_data, train_labels[label])\n",
    "    return grid\n",
    "    \n",
    "def print_Grid_SVC(grid, label):\n",
    "    print \"GridSearchCV with SVC for {}\".format(label)\n",
    "    #print \"CV Results\" \n",
    "    #print grid.cv_results_, \"\\n\"\n",
    "    print \"Best Score\"\n",
    "    print grid.best_score_, \"\\n\"\n",
    "    print \"Best Params\"\n",
    "    print grid.best_params_, \"\\n\"\n",
    "    print \"Best Estimator Index\"\n",
    "    print grid.best_index_, \"\\n\"\n",
    "    \n",
    "def print_SVC(clf,label):\n",
    "    print \"SVC for {} binary classification\".format(label)\n",
    "    print \"Number of support vectors for zero label: {}\".format(clf.n_support_[0])\n",
    "    print \"Number of support vectors for one label: {}\".format(clf.n_support_[1])\n",
    "    print \"Coefficients of support vectors in decision function\"\n",
    "    print clf.dual_coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC for jobTraining binary classification\n",
      "Number of support vectors for zero label: 834\n",
      "Number of support vectors for one label: 327\n",
      "Coefficients of support vectors in decision function\n",
      "[[-195.81478183  -76.77761956 -195.81478183 ...  335.24187236\n",
      "   222.50934884  513.03181056]]\n"
     ]
    }
   ],
   "source": [
    "#clf_job_svc = gridsearchSVC('jobTraining')    \n",
    "clf_job_svc = get_SVC(300,\"poly\") \n",
    "clf_job_svc.fit(train_data,train_labels['jobTraining'])\n",
    "print_SVC(clf_job_svc, 'jobTraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583901773533424"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = clf_job_svc.predict(train_data)\n",
    "metrics.accuracy_score(train_labels['jobTraining'], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For SVM, class probabilities computed using Platt are not theoretically sound\n",
    "#We shall extrapolate our own probabilities from the output of decision function (distance to hyperplane)\n",
    "\n",
    "def get_SVM_Proba(data,clf):\n",
    "    distances = clf.decision_function(data)\n",
    "    minimum = np.min(distances)\n",
    "    maximum = np.max(distances)\n",
    "    svc_proba = [-1.0*x/minimum if x < 0 else x/maximum for x in distances]\n",
    "    svc_proba = [0.5 + x/2.0 for x in svc_proba]\n",
    "    return distances, svc_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "distances, job_svc_proba = get_SVM_Proba(train_data,clf_job_svc)\n",
    "print np.max(job_svc_proba)\n",
    "print np.min(job_svc_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0.]\n",
      "[1. 0. 0. 0. 1. 0.]\n",
      "[ 0.38566829 -0.99972379 -0.26075574 -1.00046017  1.0002794  -0.19735857]\n",
      "[0.5894167170613608, 0.33982926549280323, 0.45822302190286124, 0.3397112862793016, 0.7319135441211728, 0.46838019855187374]\n"
     ]
    }
   ],
   "source": [
    "print train_labels['jobTraining'][0:6]\n",
    "print prediction[0:6]\n",
    "print distances[0:6]\n",
    "print job_svc_proba[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier Loss Score: 0.117331850705\n"
     ]
    }
   ],
   "source": [
    "print(\"Brier Loss Score: {}\".format(metrics.brier_score_loss(train_labels['jobTraining'], job_svc_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above value of C makes our SVM fit our model extremely closely.  \n",
    "#I don't think that is good considering we don't consider the distribution of test data or labels\n",
    "#could be somewhat different.  Also I seem not to be able to control whether L1/L2 loss is used, \n",
    "#but these are important in that they determine whether we should scale our C with the size of our data.\n",
    "#I'm just going to use value of C = 10 and kernel = poly to compute the remaining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=300, cache_size=500, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's keep C parameter high (not worried about generalization because I'm pretty sure distribution of)\n",
    "#held-out binary labels also dominated by 0\n",
    "clf_evict_svc = get_SVC(300,\"poly\") \n",
    "clf_evict_svc.fit(train_data,train_labels['eviction'])\n",
    "\n",
    "clf_layoff_svc = get_SVC(300,\"poly\")\n",
    "clf_layoff_svc.fit(train_data,train_labels['layoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, evict_prediction = get_SVM_Proba(test_data, clf_evict_svc)\n",
    "distances, layoff_prediction = get_SVM_Proba(test_data, clf_layoff_svc)\n",
    "distances, job_prediction = get_SVM_Proba(test_data, clf_job_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[-1.29108671 -1.30278921 -1.0886882  -1.69482886 -1.99947194 -0.3801412 ]\n",
      "[0.3132862261085039, 0.3115938393184857, 0.3425565993869535, 0.2548980314478593, 0.21084130126676676, 0.44502491802773947]\n"
     ]
    }
   ],
   "source": [
    "distances, evict_prob = get_SVM_Proba(train_data, clf_evict_svc)\n",
    "labels = train_labels['eviction']\n",
    "prediction = clf_evict_svc.predict(train_data)\n",
    "print labels[0:6]\n",
    "print prediction[0:6]\n",
    "print distances[0:6]\n",
    "print evict_prob[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do SVR for continuous output labels now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_ = metrics.make_scorer(metrics.mean_squared_error, greater_is_better=False)\n",
    "\n",
    "#scorer_ = metrics.make_scorer(feedToBrier, greater_is_better=False,needs_proba=True)\n",
    "\n",
    "#scorer_ = metrics.make_scorer(metrics.accuracy_score)\n",
    "\n",
    "def get_SVR(C_,kernel_): #C=1.0, kernel='rbf'\n",
    "    return svm.SVR(kernel=kernel_, degree=3, gamma='auto', coef0=0.0, tol=0.001, C=C_, \n",
    "                   epsilon=0.1, shrinking=True, cache_size=500, verbose=False, max_iter=-1)\n",
    "\n",
    "def gridsearchSVR(label):\n",
    "    params = {\"kernel\":[ \"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "           \"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    grid = GridSearchCV(get_SVR(1.0,'rbf'), params, cv=5, scoring=scorer_, refit=True, iid=True)\n",
    "    grid.fit(train_data, train_labels[label])\n",
    "    return grid\n",
    "    \n",
    "def print_Grid_SVR(grid, label):\n",
    "    print \"Support Vector Machine Regression for {}\".format(label)\n",
    "    #print \"CV Results\" \n",
    "    #print grid.cv_results_, \"\\n\"\n",
    "    print \"Best Score\"\n",
    "    print grid.best_score_, \"\\n\"\n",
    "    print \"Best Params\"\n",
    "    print grid.best_params_, \"\\n\"\n",
    "    print \"Best Estimator Index\"\n",
    "    print grid.best_index_, \"\\n\"\n",
    "\n",
    "def print_SVR(clf,label):\n",
    "    print \"SVR for regression on {}\".format(label)\n",
    "    print \"Number of support vectors\"\n",
    "    print clf.n_support_, \"\\n\"\n",
    "    print \"Coefficients of support vectors in decision function\"\n",
    "    print clf.dual_coef_, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR for regression on gpa\n",
      "Number of support vectors\n",
      "[739489952         1] \n",
      "\n",
      "Coefficients of support vectors in decision function\n",
      "[[ -17.58147095 -163.33821114 -300.         ...  136.85381364\n",
      "   -62.09595489   96.59925161]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clf_gpa_svr = gridsearchSVR('gpa') \n",
    "clf_gpa_svr = get_SVR(300,'poly') \n",
    "clf_gpa_svr.fit(train_data,train_labels['gpa'])\n",
    "print_SVR(clf_gpa_svr, 'gpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03370794418779471"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check, we should have very low MSE on \"train data\"\n",
    "prediction = clf_gpa_svr.predict(train_data)\n",
    "metrics.mean_squared_error(train_labels['gpa'], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=300, cache_size=500, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collect SVR's for the two other continuous label types\n",
    "clf_grit_svr = get_SVR(300,\"poly\") \n",
    "clf_grit_svr.fit(train_data,train_labels['grit'])\n",
    "\n",
    "clf_hard_svr = get_SVR(300,\"poly\")\n",
    "clf_hard_svr.fit(train_data,train_labels['materialHardship'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_prediction = clf_gpa_svr.predict(test_data)\n",
    "grit_prediction = clf_grit_svr.predict(test_data)\n",
    "hard_prediction = clf_hard_svr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the best results for Continous Labels (Linear Regression)\n",
    "#Combine with best results for Binary Labels (SVM - SVC)\n",
    "#Write to output_prediction.csv\n",
    "\n",
    "distances, evict_prediction = get_SVM_Proba(test_data, clf_evict_svc)\n",
    "distances, layoff_prediction = get_SVM_Proba(test_data, clf_layoff_svc)\n",
    "distances, job_prediction = get_SVM_Proba(test_data, clf_job_svc)\n",
    "gpa_prediction = clf_gpa_linear.predict(test_data)\n",
    "grit_prediction = clf_grit_linear.predict(test_data)\n",
    "hard_prediction = clf_hard_linear.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'challengeID' : [i for i in range(1,4243)]}) \n",
    "b = pd.DataFrame({'gpa' : gpa_prediction}) \n",
    "c = pd.DataFrame({'grit': grit_prediction}) \n",
    "d = pd.DataFrame({'materialHardship': hard_prediction}) \n",
    "e = pd.DataFrame({'eviction': evict_prediction}) \n",
    "f = pd.DataFrame({'layoff': layoff_prediction}) \n",
    "g = pd.DataFrame({'jobTraining': job_prediction})\n",
    "\n",
    "a = pd.concat([a, b], axis=1)\n",
    "a = pd.concat([a, c], axis=1)\n",
    "a = pd.concat([a, d], axis=1)\n",
    "a = pd.concat([a, e], axis=1)\n",
    "a = pd.concat([a, f], axis=1)\n",
    "a = pd.concat([a, g], axis=1)\n",
    "\n",
    "a.to_csv(\"output_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most Important Features From Regression Techniques\n",
    "\n",
    "log_l2_job = [849, 416, 576, 81, 49, 852, 343, 657, 13, 844]\n",
    "log_l1_job = [445, 176, 841, 160, 343, 852, 13, 814, 844, 797] \n",
    "\n",
    "log_l2_evict = [722, 407, 637, 764, 674, 804, 805, 329, 461, 476]\n",
    "log_l1_evict = [569, 382, 743, 13, 264, 65, 845, 176, 637, 686] \n",
    "\n",
    "log_l2_layoff = [359, 563, 164, 277, 264, 569, 591, 11, 49, 13]\n",
    "log_l1_layoff = [283, 284, 285, 286, 287, 288, 289, 290, 292, 854]\n",
    "\n",
    "lin_l1_gpa = [556, 851, 430, 534, 154, 16, 682, 657, 647, 816]\n",
    "lin_l2_grit = [410, 233, 383, 55, 230, 154, 254, 49, 33, 593]\n",
    "lin_l2_hard = [81, 255, 45, 416, 673, 844, 384, 594, 13, 406]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert indexes to actual var name\n",
    "\n",
    "all_columns = list(uc_columns)\n",
    "\n",
    "def extract_vars(indexes):\n",
    "    var_names = []\n",
    "    for i in indexes:\n",
    "        if i <= 641:\n",
    "            var_names.append(uc_columns[indexToVar[i]])\n",
    "        else:\n",
    "            var_names.append(other_columns[i-642])\n",
    "    return var_names\n",
    "        \n",
    "log_l2_job_vars = extract_vars(log_l2_job)\n",
    "log_l1_job_vars = extract_vars(log_l1_job)\n",
    "log_l2_evict_vars = extract_vars(log_l2_evict)\n",
    "log_l1_evict_vars = extract_vars(log_l1_evict)\n",
    "log_l2_layoff_vars = extract_vars(log_l2_layoff)\n",
    "log_l1_layoff_vars = extract_vars(log_l1_layoff)\n",
    "lin_l1_gpa_vars = extract_vars(lin_l1_gpa)\n",
    "lin_l2_grit_vars = extract_vars(lin_l2_grit)\n",
    "lin_l2_hard_vars = extract_vars(lin_l2_hard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_frequency(vars_list):\n",
    "    temp_dict = {}\n",
    "    for var_list in vars_list:\n",
    "        for var in var_list:\n",
    "            if var in temp_dict:\n",
    "                temp_dict[var] = temp_dict[var] + 1\n",
    "            else:\n",
    "                temp_dict[var] = 1\n",
    "                \n",
    "    temp_list = []\n",
    "\n",
    "    for var in temp_dict:\n",
    "        temp_list.append((var,temp_dict[var]))\n",
    "\n",
    "    temp_list = sorted(temp_list, key=lambda x: x[1])\n",
    "    return temp_list\n",
    "\n",
    "log_job_vars = sort_by_frequency([log_l2_job_vars, log_l1_job_vars])\n",
    "log_evict_vars = sort_by_frequency([log_l2_evict_vars, log_l1_evict_vars])\n",
    "log_layoff_vars = sort_by_frequency([log_l2_layoff_vars, log_l1_layoff_vars])\n",
    "lin_gpa_vars = lin_l1_gpa_vars\n",
    "lin_grit_vars = lin_l2_grit_vars\n",
    "lin_hard_vars = lin_l2_hard_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Important Features for Job Training\n",
      "[('ct4kyear', 1), ('cm2hhimp', 1), ('cm1edu', 1), ('cm1ethrace', 1), ('cm5b_ageyrs', 1), ('ch4haz', 1), ('cf2cohm', 1), ('cm5povco', 1), ('cm5marp', 1), ('ch4htwt', 1), ('ch5dsss', 1), ('cf3samp', 1), ('cf4cohp', 2), ('cf1ethrace', 2), ('cm5edu', 2), ('cm5povca', 2)] \n",
      "\n",
      "Most Important Features for Eviction\n",
      "[('ch3waz', 1), ('cm4hhimp', 1), ('cm4fdiff', 1), ('cm3span', 1), ('cm5relf', 1), ('ch3att_v', 1), ('ch4mwtkg', 1), ('ch4selfht', 1), ('cm2amrf', 1), ('cf1ethrace', 1), ('cf4samp', 1), ('cm5bmomstat', 1), ('cm4kids', 1), ('cf5edu', 1), ('cf2mint', 1), ('cf3kids', 1), ('cf3samp', 1), ('ch4mwtlb', 1), ('cf5samp', 2)] \n",
      "\n",
      "Most Important Features for Layoff\n",
      "[('cm3span', 1), ('cm5md_case_con', 1), ('cm1ethrace', 1), ('cm2span', 1), ('cf1gmom', 1), ('ch3ovscale', 1), ('cm5span', 1), ('cm5relf', 1), ('cf1ethrace', 1), ('ch3mompreg', 1), ('cf5povcab', 1), ('cf4md_case_lib', 1), ('ch3mesyr', 1), ('ch3cflag', 2), ('ch3cwtalone', 2), ('ch3mflag', 3)] \n",
      "\n",
      "Most Important Features for GPA\n",
      "['cf5tele', 'cf5povcob', 'ch4mesyr', 'ch5selfht', 'cm2md_case_lib', 'cf1hhimp', 'cm2povca', 'cm1edu', 'cf1edu', 'ch5ppvtraw'] \n",
      "\n",
      "Most Important Features for Grit\n",
      "['cm4hhimp', 'cm3cohf', 'cm4inhom', 'cm1hhimp', 'cm3marf', 'cm2md_case_lib', 'cm3md_case_lib', 'cm1ethrace', 'cm1bsex', 'cm5md_case_lib'] \n",
      "\n",
      "Most Important Features for Material Hardship\n",
      "['cf2cohm', 'cm3md_case_lib', 'cm1gdad', 'ct4kyear', 'cm2relf', 'cm5edu', 'cm4inhom', 'cm5md_case_lib', 'cf1ethrace', 'cm4md_case_lib']\n"
     ]
    }
   ],
   "source": [
    "print \"Most Important Features for Job Training\"\n",
    "print log_job_vars, \"\\n\"\n",
    "print \"Most Important Features for Eviction\"\n",
    "print log_evict_vars, \"\\n\"\n",
    "print \"Most Important Features for Layoff\"\n",
    "print log_layoff_vars, \"\\n\"\n",
    "print \"Most Important Features for GPA\"\n",
    "print lin_gpa_vars, \"\\n\"\n",
    "print \"Most Important Features for Grit\"\n",
    "print lin_grit_vars, \"\\n\"\n",
    "print \"Most Important Features for Material Hardship\"\n",
    "print lin_hard_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mental health'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_csv.loc[meta_csv.loc[:,'new_name'] == 'cm5md_case_lib','topic1'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(var_list, _tuple):\n",
    "    temp_dict = {}\n",
    "    if _tuple:\n",
    "        for var in var_list:\n",
    "            temp_dict[meta_csv.loc[meta_csv.loc[:,'new_name'] == var[0],'topic1'].iloc[0]] = True \n",
    "    else:\n",
    "        for var in var_list:\n",
    "            temp_dict[meta_csv.loc[meta_csv.loc[:,'new_name'] == var,'topic1'].iloc[0]] = True \n",
    "\n",
    "    return temp_dict\n",
    "\n",
    "log_job_topics = get_topics(log_job_vars, True)\n",
    "log_evict_topics = get_topics(log_evict_vars, True)\n",
    "log_layoff_topics = get_topics(log_layoff_vars, True)\n",
    "lin_gpa_topics = get_topics(lin_gpa_vars, False)\n",
    "lin_grit_topics = get_topics(lin_grit_vars, False)\n",
    "lin_hard_topics = get_topics(lin_hard_vars, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Important Feature Topics for Job Training\n",
      "['cognitive skills', 'height and weight', 'educational attainment/achievement', 'parental relationship status', 'age', 'race/ethnicity', 'new partner relationship status', 'household income/poverty', 'paradata'] \n",
      "\n",
      "Most Important Feature Topics for Eviction\n",
      "['height and weight', 'educational attainment/achievement', 'parental relationship status', 'age', 'household composition', 'race/ethnicity', 'behavior', 'household income/poverty', 'paradata'] \n",
      "\n",
      "Most Important Feature Topics for Layoff\n",
      "['mental health', 'height and weight', 'parental relationship status', 'paradata', 'household composition', 'race/ethnicity', 'fertility history', 'household income/poverty'] \n",
      "\n",
      "Most Important Feature Topics for GPA\n",
      "['cognitive skills', 'height and weight', 'educational attainment/achievement', 'paradata', 'mental health', 'household income/poverty'] \n",
      "\n",
      "Most Important Feature Topics for Grit\n",
      "['mental health', 'sex/gender', 'parental relationship status', 'paradata', 'race/ethnicity', 'household income/poverty'] \n",
      "\n",
      "Most Important Feature Topics for Material Hardship\n",
      "['mental health', 'educational attainment/achievement', 'parental relationship status', 'paradata', 'household composition', 'race/ethnicity']\n"
     ]
    }
   ],
   "source": [
    "print \"Most Important Feature Topics for Job Training\"\n",
    "print log_job_topics.keys(), \"\\n\"\n",
    "print \"Most Important Feature Topics for Eviction\"\n",
    "print log_evict_topics.keys(), \"\\n\"\n",
    "print \"Most Important Feature Topics for Layoff\"\n",
    "print log_layoff_topics.keys(), \"\\n\"\n",
    "print \"Most Important Feature Topics for GPA\"\n",
    "print lin_gpa_topics.keys(), \"\\n\"\n",
    "print \"Most Important Feature Topics for Grit\"\n",
    "print lin_grit_topics.keys(), \"\\n\"\n",
    "print \"Most Important Feature Topics for Material Hardship\"\n",
    "print lin_hard_topics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
